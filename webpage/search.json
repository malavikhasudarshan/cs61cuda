[
  {
    "objectID": "webpage.html",
    "href": "webpage.html",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "",
    "text": "Welcome to CS61Cuda!! This is a mini‑project that introduces you to GPU programming with CUDA by building up to a fast matrix multiply. You’ll start with a CPU reference, write your first CUDA kernels, learn to reason about grids/blocks/threads, and finally add simple vectorization (SIMD) on the GPU. An optional performance sandbox lets you explore optimizations for bragging rights."
  },
  {
    "objectID": "webpage.html#learning-goals",
    "href": "webpage.html#learning-goals",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "1 Learning Goals",
    "text": "1 Learning Goals\n\nMap data‑parallel work to CUDA’s grid/block/thread hierarchy.\n\nPractice indexing and bounds checks in 1D/2D.\n\nUnderstand memory access patterns (coalescing) and why they matter.\n\nSee the benefits of TLP (thread‑level parallelism) and DLP (data‑level/SIMD) on the GPU.\n\nRead simple performance counters and reason about memory‑ vs compute‑bound kernels."
  },
  {
    "objectID": "webpage.html#repo-layout-what-you-edit",
    "href": "webpage.html#repo-layout-what-you-edit",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "2 Repo Layout & What You Edit",
    "text": "2 Repo Layout & What You Edit\ncs61cuda/\n├─ CMakeLists.txt # or Makefile (both provided)\n├─ include/\n│ └─ matmul.h # shared function prototypes\n├─ src/\n│ ├─ main.cpp # driver: parses flags, allocs, calls your code\n│ ├─ task_1_cuda_copy.cu # ✏️ Task 1: 2D copy kernel\n│ ├─ task_2_cpu_baseline.cpp # ✏️ Task 2: implement CPU matmul\n│ ├─ task_3_cuda_naive.cu # ✏️ Task 3: naive CUDA matmul (1 output/thread)\n│ ├─ task_4_cuda_simd.cu # ✏️ Task 4: vectorized CUDA matmul (no shared mem)\n│ ├─ utils.cu # timers, error checks, random init, compare\n│ └─ check.cuh # CUDA error macros (provided)\n├─ tests/\n│ ├─ correctness_tests.py # local correctness checks\n│ └─ perf_runner.py # runs sizes & prints throughput\n├─ data/\n│ └─ generate.py # makes small sample matrices (optional)\n├─ scripts/\n│ ├─ build.sh # nvcc or cmake build helper\n│ └─ run_all.sh # runs all tasks & tests\n└─ README.md # quick overview\nYou will edit: src/task_1_cpu_baseline.cpp, src/task_2_cuda_copy.cu, src/task_3_cuda_naive.cu, src/ctask_4_cuda_simd.cu."
  },
  {
    "objectID": "webpage.html#grading-summary",
    "href": "webpage.html#grading-summary",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "3 Grading Summary",
    "text": "3 Grading Summary\n\n\n\n\n\n\n\n\nTask\nDescription\nPoints\n\n\n\n\n1\nWelcome to 61Cuda: Copy kernel (2D grid, indexing, bounds, coalescing)\n10\n\n\n2\nBaseline CPU matmul (triple-loop reference)\n15\n\n\n3\nCUDA naive matmul (1 output/thread)\n30\n\n\n4\nCUDA SIMD matmul (vectorized loads, no shared memory)\n30\n\n\n5\nPerformance engineering (optional)\n15 EC\n\n\n\nTotal: 100 required + 15 extra credit.\nAutograder policy. We check correctness on hidden sizes, run times on a standard GPU, and basic style (clear bounds checks, no UB). We do not require a specific speedup, but we verify your kernels scale sensibly with size.\nCollaboration. Discuss ideas high‑level with peers; code must be your own. Cite any online sources you consulted."
  },
  {
    "objectID": "webpage.html#cuda-primer-read-first",
    "href": "webpage.html#cuda-primer-read-first",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "4 CUDA Primer (read first)",
    "text": "4 CUDA Primer (read first)\n\n4.1 Core Concepts\n\nA kernel is a C/C++ function annotated __global__ and launched with &lt;&lt;&lt;grid, block&gt;&gt;&gt;.\n\nEach launch creates a 2‑D/3‑D grid of blocks; each block contains many threads. Every thread runs the same kernel on different data.\n\nExample: myKernel&lt;&lt;&lt;dim3(2,2), dim3(16,16)&gt;&gt;&gt;(args) creates a 2×2 grid of blocks, each with 16×16 threads (total: 4 blocks × 256 threads = 1024 threads).\n\nBuilt‑in variables inside kernels: blockIdx.{x,y,z}, threadIdx.{x,y,z}, blockDim.{x,y,z}, gridDim.{x,y,z}.\n\n2D Indexing Example:\nint i = blockIdx.y * blockDim.y + threadIdx.y;  // row\nint j = blockIdx.x * blockDim.x + threadIdx.x;  // column\nGrid size calculation: When dimensions don’t divide evenly, round up:\ndim3 grid((width + block.x - 1) / block.x, (height + block.y - 1) / block.y);\n\n\n4.2 Memory Hierarchy\n\nGlobal (device DRAM): large, high‑latency (~400-800 cycles); visible to all threads. Allocated with cudaMalloc(), accessed via pointers. Persists across kernel launches.\nShared (on‑chip, per‑block): small (~48KB per SM), low‑latency (~20-30 cycles); visible to threads in the same block. Declared with __shared__. Cleared between kernel launches.\nRegisters (per‑thread): fastest (~1 cycle), limited (~255 per thread). Automatic for local variables. Private to each thread.\n\nMemory transfer patterns: * Host → Device: cudaMemcpy(dst, src, size, cudaMemcpyHostToDevice) * Device → Host: cudaMemcpy(dst, src, size, cudaMemcpyDeviceToHost) * Device → Device: cudaMemcpy(dst, src, size, cudaMemcpyDeviceToDevice)\n\n\n4.3 Performance Concepts\n\nMemory coalescing: threads in a warp (32 threads) should access consecutive addresses to combine loads/stores into few transactions.\n\n✅ Good: thread 0 → addr[0], thread 1 → addr[1], ..., thread 31 → addr[31] (coalesced into 1-2 transactions)\n\n❌ Bad: thread 0 → addr[0], thread 1 → addr[100], thread 2 → addr[200]... (32 separate transactions)\n\nWarp: A group of 32 threads that execute in lockstep (SIMT - Single Instruction, Multiple Threads). Threads in a warp share the same instruction stream. Warps are the fundamental unit of execution on the GPU.\nSynchronization: __syncthreads() is a barrier for all threads in a block (not across blocks). Use before reading shared memory written by other threads. Important: All threads in a block must reach the barrier, or you’ll deadlock.\nOccupancy: The ratio of active warps to maximum warps per SM. Higher occupancy doesn’t always mean better performance (register pressure, shared memory limits).\n\n\n\n4.4 Common Patterns & Best Practices\nYou’ll always (1) map data to threads, (2) ensure bounds checks, (3) choose grid/block sizes, and (4) verify results.\nBounds checking pattern:\nint idx = blockIdx.x * blockDim.x + threadIdx.x;\nif (idx &gt;= N) return;  // Guard against out-of-bounds\nBlock size guidelines: * Use multiples of 32 (warp size): 32, 64, 128, 256, 512, 1024 * For 2D: Common sizes are 8×8, 16×16, 32×32 * Balance: Larger blocks = more shared memory per block, but fewer blocks = less parallelism\n\n\n4.5 Common Pitfalls\n\nForgetting bounds checks: Always check if (i &gt;= rows || j &gt;= cols) return; when grid size rounds up.\nWrong indexing order: Use threadIdx.x for columns (fastest dimension) to enable coalescing in row-major layouts.\nMissing synchronization: If threads write then read shared memory, use __syncthreads() between.\nNot checking errors: Always call checkCuda(cudaGetLastError()) after kernel launches.\nRace conditions: Multiple threads writing to the same global memory location without atomics.\nRegister spilling: Too many local variables can cause register spilling to local memory (slow).\n\n\n\n4.6 Debugging Tips\n\nStart small: Test with 8×8 matrices before scaling up.\nUse printf in kernels (sparingly) for small sizes: if (threadIdx.x == 0 && blockIdx.x == 0) printf(\"value: %f\\n\", x);\nRun cuda-memcheck: cuda-memcheck ./your_program to catch memory errors, race conditions, and invalid accesses.\nCheck occupancy: Use nvidia-smi to monitor GPU usage, or profiling tools if available.\nSynchronize before copying: Use cudaDeviceSynchronize() before copying results back to host.\nVerify with small inputs: Always test correctness on small, known inputs first.\n\n\n\n4.7 CUDA Documentation & Finding Functions\nYou will need to look up CUDA functions in the official documentation. For each function you use:\n\nIf the function has its own page (e.g., cudaMalloc, cudaMemcpy), link directly to that page.\nIf the function is part of a larger API section (e.g., vector types like float4), link to the relevant section of the programming guide.\n\nKey Documentation Resources:\n\nCUDA C++ Programming Guide - Complete reference for CUDA programming\n\nMemory Hierarchy - Detailed memory model\nBuilt-in Variables - blockIdx, threadIdx, etc.\nVector Types - float4, int4, etc.\nSynchronization Functions - __syncthreads(), etc.\n\nCUDA Runtime API - Host-side functions\n\nMemory Management - cudaMalloc, cudaFree, cudaMemcpy\nDevice Management - cudaDeviceSynchronize, cudaGetLastError\n\nCUDA Best Practices Guide - Performance optimization tips\n\nExample: If you use cudaMalloc, link to: https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g37d37965bfb4803b6d4e59ff26856356\nIf you use float4, link to: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#vector-types\n\n\n4.8 Quick Reference\n\n\n\n\n\n\n\n\nConcept\nSyntax\nNotes\n\n\n\n\nKernel launch\nkernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(args)\nGrid/block are dim3\n\n\nGlobal memory alloc\ncudaMalloc(&ptr, size)\nReturns device pointer\n\n\nCopy to device\ncudaMemcpy(dst, src, size, cudaMemcpyHostToDevice)\n\n\n\nCopy from device\ncudaMemcpy(dst, src, size, cudaMemcpyDeviceToHost)\n\n\n\nShared memory\n__shared__ float sdata[256]\nPer-block, ~48KB limit\n\n\nSynchronization\n__syncthreads()\nBlock-level barrier\n\n\nError check\ncheckCuda(cudaGetLastError())\nAfter kernel launch\n\n\nDevice sync\ncudaDeviceSynchronize()\nWait for kernel completion\n\n\nVector load\nfloat4 vec = *reinterpret_cast&lt;float4*&gt;(&ptr[i])\nAligned access\n\n\nVector store\n*reinterpret_cast&lt;float4*&gt;(&ptr[i]) = vec\nAligned access"
  },
  {
    "objectID": "webpage.html#task-1-welcome-to-61cuda-2d-copy-kernel-10-pts",
    "href": "webpage.html#task-1-welcome-to-61cuda-2d-copy-kernel-10-pts",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "5 Task 1 — Welcome to 61Cuda: 2D Copy Kernel (10 pts)",
    "text": "5 Task 1 — Welcome to 61Cuda: 2D Copy Kernel (10 pts)\n\n5.1 Conceptual Overview\nWarm up with indexing and coalesced global memory access. You’ll copy a dense matrix from device input to device output using a 2D grid of 2D blocks, one element per thread.\n\n\n5.2 Data Flow\n\nInput: in ∈ R^{rows×cols} (row‑major in[r*cols + c]), allocated in device memory.\n\nProcessing: For each (r,c) covered by a thread, read in[r,c] and write it to out[r,c]. Use 2‑D indexing and guard against out‑of‑bounds.\n\nOutput: out ∈ R^{rows×cols} identical to in (device memory).\n\n\n\n5.3 Your Task\nImplement copy2D_kernel and the host wrapper copy2D(...)\n#include \"check.cuh\"\n\n__global__ void copy2D_kernel(const float* __restrict__ in,\nfloat* __restrict__ out,\nint rows, int cols) {\n// TODO: compute r, c using blockIdx/threadIdx and blockDim\n// TODO: bounds check: if (r &gt;= rows || c &gt;= cols) return;\n// TODO: copy one element\n}\n\nvoid copy2D(const float* d_in, float* d_out, int rows, int cols, dim3 block){\n// Suggest block = (16,16,1)\ndim3 grid((cols + block.x - 1)/block.x,\n(rows + block.y - 1)/block.y);\ncopy2D_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_in, d_out, rows, cols);\ncheckCuda(cudaGetLastError());\ncheckCuda(cudaDeviceSynchronize());\n}\nTesting & Tips\n\nPrefer threadIdx.x to index columns to encourage coalesced row‑major accesses.\n\nRun: ./build/cs61cuda --task=copy --M=64 --N=128 --verify."
  },
  {
    "objectID": "webpage.html#task-2-cpu-baseline-matmul-15-pts",
    "href": "webpage.html#task-2-cpu-baseline-matmul-15-pts",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "6 Task 2 — CPU Baseline Matmul (15 pts)",
    "text": "6 Task 2 — CPU Baseline Matmul (15 pts)\n\n6.1 Conceptual Overview\nImplement a correct triple‑loop matrix multiply in row‑major order. This is the correctness oracle for later tasks.\n\n\n6.2 Data Flow\n\nInput: A ∈ R^{M×K}, B ∈ R^{K×N} (host memory, row‑major).\n\nProcessing: For every (i,j), compute C[i,j] = Σ_{k=0..K-1} A[i,k] * B[k,j].\n\nOutput: C ∈ R^{M×N} (host memory).\n\n\n\n6.3 Your Task\nFill mm_cpu(...) in src/cpu_baseline.cpp.\nStarter (skeleton) — src/cpu_baseline.cpp\n#include &lt;cstddef&gt;\nvoid mm_cpu(const float* A, const float* B, float* C,\nint M, int N, int K){\n// TODO: triple nested loops over i (rows of A), j (cols of B), k (shared dim)\n// Use row-major: A[i*K + k], B[k*N + j], C[i*N + j]\n}\nTesting & Tips\n\nUse small sizes first (e.g., 8×8×8). GPU results in later tasks are compared against this function with tolerance 1e‑4."
  },
  {
    "objectID": "webpage.html#task-3-cuda-naive-matmul-30-pts",
    "href": "webpage.html#task-3-cuda-naive-matmul-30-pts",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "7 Task 3 — CUDA Naive Matmul (30 pts)",
    "text": "7 Task 3 — CUDA Naive Matmul (30 pts)\n\n7.1 Conceptual Overview\nParallelize Task 2 on the GPU: map each output element C[i,j] to a single thread. This exposes thread‑level parallelism (TLP); performance is often limited by memory bandwidth.\n\n\n7.2 Data Flow\n\nInput: A ∈ R^{M×K}, B ∈ R^{K×N} (device memory), populated from host.\n\nProcessing: Each thread computes one output C[i,j] by streaming the k dimension.\n\nOutput: C ∈ R^{M×N} (device memory), copied back to host by the driver when verifying.\n\n\n\n7.3 Your Task\nImplement mm_naive_kernel and the host launcher mm_naive(...).\nStarter (skeleton) — src/cuda_naive.cu\n#include \"check.cuh\"\n\n__global__ void mm_naive_kernel(const float* __restrict__ A,\nconst float* __restrict__ B,\nfloat* __restrict__ C,\nint M, int N, int K){\n// TODO: compute i (row) and j (col) from 2D grid/block\n// TODO: bounds guard\n// TODO: accumulate over k\n}\n\nvoid mm_naive(const float* dA, const float* dB, float* dC,\nint M, int N, int K, dim3 block){\ndim3 grid((N + block.x - 1)/block.x,\n(M + block.y - 1)/block.y);\nmm_naive_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(dA, dB, dC, M, N, K);\ncheckCuda(cudaGetLastError());\ncheckCuda(cudaDeviceSynchronize());\n}\nNotes on memory access\n\nThreads in a warp vary j at fixed k, so B[k*N + j] is coalesced. Accesses to A[i*K + k] replicate a single element per thread (cache helps).\n\nRun ./build/cs61cuda --task=naive --M=512 --N=512 --K=512 --verify"
  },
  {
    "objectID": "webpage.html#task-4-cuda-simd-matmul-vectorized-no-shared-memory-30-pts",
    "href": "webpage.html#task-4-cuda-simd-matmul-vectorized-no-shared-memory-30-pts",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "8 Task 4 — CUDA SIMD Matmul (Vectorized, No Shared Memory) (30 pts)",
    "text": "8 Task 4 — CUDA SIMD Matmul (Vectorized, No Shared Memory) (30 pts)\n\n8.1 Conceptual Overview\nAugment TLP with simple data‑level parallelism (DLP): each thread computes a short contiguous vector of outputs in a row using vector loads/stores (e.g., float4). No shared memory yet.\n\n\n8.2 Data Flow\n\nInput: A ∈ R^{M×K}, B ∈ R^{K×N} (device).\n\nProcessing: A thread at (i, j0_group) produces V outputs C[i, j0..j0+V-1]. Inner loop streams over k, reading one scalar A[i,k] and a vector of V neighbors from row B[k, :].\n\nOutput: C ∈ R^{M×N} (device).\n\n\n\n8.3 Your Task\nImplement mm_simd_kernel&lt;V&gt;() and its launcher. Default V=4; handle tails where N % V ≠ 0.\nStarter (skeleton) — src/cuda_simd.cu\n#include \"check.cuh\"\n\ntemplate&lt;int V&gt;\n__global__ void mm_simd_kernel(const float* __restrict__ A,\nconst float* __restrict__ B,\nfloat* __restrict__ C,\nint M, int N, int K){\n// TODO: compute i (row) and j0 (first column of this thread's vector)\n// TODO: maintain acc[V] and handle aligned vs tail paths\n}\n\nvoid mm_simd(const float* dA, const float* dB, float* dC,\nint M, int N, int K, int vec, dim3 block){\n// TODO: call specialized kernel for vec==4, else fall back\n}\nRun ./build/cs61cuda --task=simd --M=1024 --N=1024 --K=1024 --vec=4 --verify\nDiscussion prompts\n\nWhy does vectorizing along columns improve coalescing for loads from B and stores to C?\n\nWhat changes would shared‑memory tiling introduce (Task 5 EC)?"
  },
  {
    "objectID": "webpage.html#task-5-optional-performance-engineering-15-ec",
    "href": "webpage.html#task-5-optional-performance-engineering-15-ec",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "9 Task 5 — (Optional) Performance Engineering (15 EC)",
    "text": "9 Task 5 — (Optional) Performance Engineering (15 EC)\nMake it faster. Ideas:\n\nShared‑memory tiling (classic 16×16 or 32×32 tiles).\n\nRegister tiling: each thread computes a small r×c tile.\n\nLoop unrolling & #pragma unroll for k.\n\nOccupancy tuning: vary blockDim to trade registers vs parallelism.\n\nMixed precision: keep FP32 accumulate but try __half inputs (only if you also keep a FP32 correctness path for grading).\n\nSoftware prefetch: read the next k slice early.\n\n\n9.1 Performance Targets & Grading\nWe’ll measure performance in GFLOP/s (billions of floating-point operations per second) on a standard GPU. The formula is: GFLOP/s = (2 × M × N × K) / (time_in_seconds × 1e9).\nGrading Rubric:\n\nFull credit (15 pts): ≥ 2.0× speedup over Task 4 baseline\n\nExample: If Task 4 runs at 500 GFLOP/s, you need ≥ 1000 GFLOP/s\nMust maintain correctness (within 1e-4 tolerance)\nMust include analysis in README-perf.md\n\nPartial credit (10 pts): 1.5× - 2.0× speedup over Task 4 baseline\n\nExample: If Task 4 runs at 500 GFLOP/s, you need 750-1000 GFLOP/s\nMust maintain correctness\nMust include analysis in README-perf.md\n\nMinimal credit (5 pts): 1.2× - 1.5× speedup over Task 4 baseline\n\nExample: If Task 4 runs at 500 GFLOP/s, you need 600-750 GFLOP/s\nMust maintain correctness\nMust include analysis in README-perf.md\n\nNo credit (0 pts): &lt; 1.2× speedup, or correctness failures, or missing README-perf.md\n\nNote: Baseline Task 4 performance will vary by GPU. We’ll normalize based on the reference implementation’s performance on the grading hardware. The speedup ratios above are relative to your Task 4 implementation.\nWe’ll publish a lightweight leaderboard (GFLOP/s). Please write a short README-perf.md describing: * What optimizations you tried * Why each optimization helped (or didn’t) * Performance measurements (GFLOP/s) for Task 4 baseline and your optimized version * Any insights about memory access patterns, occupancy, or other performance factors\nCommand‑Line Interface (Driver)\n./cs61cuda --task={copy|cpu|naive|simd}\n--M=1024 --N=1024 --K=1024\n--block=16 --vec=4 --repeat=10 --verify\n\n--task=copy ignores K.\n\n--vec controls SIMD width in Task 4; grader uses 4.\n\n--verify runs mm_cpu then compares results (L2 relative error ≤ 1e-4)."
  },
  {
    "objectID": "webpage.html#correctness-floatingpoint-notes",
    "href": "webpage.html#correctness-floatingpoint-notes",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "10 Correctness & Floating‑Point Notes",
    "text": "10 Correctness & Floating‑Point Notes\n\nWe compare with a small absolute+relative tolerance (1e-4).\n\nRandom inputs in [-1, 1] with fixed seed.\n\nYour GPU kernels must not read/write out of bounds; failing cuda-memcheck is an automatic zero for that test."
  },
  {
    "objectID": "webpage.html#debugging-checklist",
    "href": "webpage.html#debugging-checklist",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "11 Debugging Checklist",
    "text": "11 Debugging Checklist\n\nAfter each kernel launch:\n\ncudaDeviceSynchronize();\ncheckCuda(cudaGetLastError());\n\nUse printf inside kernels sparingly on tiny sizes.\n\nTry cuda-memcheck for invalid addresses / race conditions.\n\nStart small (e.g., M=N=K=8) then scale."
  },
  {
    "objectID": "webpage.html#style-submission",
    "href": "webpage.html#style-submission",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "12 Style & Submission",
    "text": "12 Style & Submission\n\nClear variable names (i,j,k, M,N,K).\n\nComments: what the mapping is, what each thread computes, any assumptions.\n\nNo undefined behavior (no out‑of‑bounds pointer math, no aliasing shenanigans).\n\nSubmit your edited .cpp/.cu files and README.md answering the reflection prompts below.\n\n\n12.1 Reflection (graded in Task 5 rubric even if you skip perf EC)\n\nWhere is your naive kernel memory‑bound? Which array dominates traffic and why?\n\nWhy does vectorizing along columns improve coalescing? What’s the trade‑off?\n\nWhat would shared‑memory tiling change about the access pattern?"
  },
  {
    "objectID": "webpage.html#reference-shapes-indexing",
    "href": "webpage.html#reference-shapes-indexing",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "13 Reference: Shapes & Indexing",
    "text": "13 Reference: Shapes & Indexing\nRow‑major:\nA: MxK → A[i*K + k]\nB: KxN → B[k*N + j]\nC: MxN → C[i*N + j]\nGrid/block formulas used throughout:\nint i = blockIdx.y * blockDim.y + threadIdx.y;\nint j = blockIdx.x * blockDim.x + threadIdx.x;"
  },
  {
    "objectID": "webpage.html#rubric-details",
    "href": "webpage.html#rubric-details",
    "title": "CS61Cuda Project: Matmul & CUDA Fundamentals",
    "section": "14 Rubric Details",
    "text": "14 Rubric Details\nTask 1 (10)\n\n\ncorrect 2D indexing + bounds\n\n\n\ncoalesced mapping (x→cols)\n\n\n\npasses tests, tidy style\n\n\nTask 2 (15)\n\n\ncorrect triple‑loop for arbitrary M,N,K\n\n\n\nclear comments & no UB\n\n\nTask 3 (30)\n\n\ncorrect one‑element/thread mapping\n\n\n\ncorrect launch geometry & bounds\n\n\n\nreasonable performance (scaled timing)\n\n\n\ncomments explaining memory pattern\n\n\nTask 4 (30)\n\n\ncorrect SIMD (V outputs/thread), handles tails\n\n\n\nproper vector loads/stores when aligned\n\n\n\ncoalesced writes & justified mapping\n\n\n\nperformance better than naive on large sizes\n\n\nTask 5 EC (15)\n\n\nmeasurable speedup over Task 4\n\n\n\nREADME‑perf.md analysis"
  }
]